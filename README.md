
# ğŸ“ˆ META Ads Reporting and Analysis Tool

**Meta Ads Reporting and Analysis Tool**â€”a cloud-powered pipeline that transforms raw META (Facebook/Instagram) ad exports into structured insights and business-ready reports.

---

## ğŸš€ Whatâ€™s This Platform For?

This platform is built to help marketers and analysts extract **meaningful insights** from recurring ad campaign data. Whether youâ€™re tracking *quote rates*, *cost-per-result*, or *Instagram profile visits*, this end-to-end pipeline handles it allâ€”automatically.

Use it to:
- ğŸ’¡ Understand asset-level ad performance across time.
- ğŸ“Š Compare campaigns via SQL-powered analysis.
- ğŸ§º Store structured datasets in a cloud-based RDS.
- ğŸ“ Generate weekly/monthly reports for decision-makers.

---

## ğŸ› ï¸ Platform Architecture

```
META Ads Export (.csv)
        â¬‡
    âœ… Upload to S3
        â¬‡
ğŸª‚ AWS Lambda Triggered
        â¬‡
ğŸ¤– SageMaker Processing Job
        â¬‡
ğŸ“Š PostgreSQL (RDS) Database
        â¬‡
ğŸ“¤ SQL-Based Analysis + Reports
        â¬‡
ğŸ“ CSV Reports (with timestamps)
```

---

## â˜ï¸ Cloud Tools Used

| Service           | Role                                                          |
|------------------|---------------------------------------------------------------|
| **S3**           | Stores raw and preprocessed ad data files                     |
| **Lambda**       | Detects new uploads and triggers processing jobs              |
| **SageMaker**    | Runs Python scripts for data cleaning + analysis              |
| **PostgreSQL RDS** | Stores structured campaign data for SQL-based querying        |
| **QuickSight / Power BI** *(optional)* | Visualizes data over time                            |

---

## ğŸ’» Local Usage Option

You can also run the pipeline **locally**â€”no cloud setup needed.

Just make sure:
- Your `data/` folder contains valid `weekly_data/` and `monthly_data/` CSVs matching the expected schema.
- Youâ€™ve installed all dependencies using `requirements.txt`.
- You have a running PostgreSQL instance (local or remote) and itâ€™s connected properly.

Then, trigger the pipeline manually with:
```bash
python main.py
```

All analysis and reports will be generated exactly like they would in the cloud setup.

---

## ğŸ“¦ Project Structure

```
ğŸ“ Meta-Ads-Reporting-and-Analysis-Tool/
â”œâ”€â”€ analysis/                  # Local data folder (optional)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ monthly_data/          # Monthly data in CSV format
â”‚   â””â”€â”€ weekly_data/           # Weekly data in CSV format
â”œâ”€â”€ reports/                   # Generated .txt + .png reports
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessor/
â”‚   â”‚   â”œâ”€â”€ core.py            # Main orchestrator
â”‚   â”‚   â””â”€â”€ steps/             # Modular cleaning functions
â”‚   â”œâ”€â”€ analyser/
â”‚   â”‚   â”œâ”€â”€ core.py            # SQL analysis orchestration
â”‚   â”‚   â””â”€â”€ sql_metrics.py     # Defined SQL-based metrics
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â””â”€â”€ save.py            # Database operations
â”‚   â””â”€â”€ reporter/
â”‚       â””â”€â”€ generate_reports.py # Report generation logic
â”œâ”€â”€ EDA.ipynb                  # EDA + experimentation
â”œâ”€â”€ requirements.txt           # Pip dependencies
â””â”€â”€ main.py                    # Pipeline entry point
```

---

## ğŸ§ª Installation & Setup

> **Pre-req:** Python 3.10+, PostgreSQL running (AWS RDS or local), AWS CLI configured

1. **Clone the repo:**
   ```bash
   git clone https://github.com/yourusername/ad-data-analysis.git
   cd ad-data-analysis
   ```

2. **Install dependencies:**
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # Or .venv\Scripts\activate on Windows
   pip install -r requirements.txt
   ```

3. **Set environment variables:**
   Either create a `.env` file or configure AWS credentials + DB URI manually in your code.

4. **Trigger the pipeline:**
   - **Manual:** Run `main.py` after placing raw `.csv` in your S3 bucket or local `data/` folder.
   - **Auto (AWS):** Use your Lambda trigger (already preconfigured) to auto-process uploads.

---

## ğŸ“Š What Does It Analyze?

Metrics currently supported:
- **Cost Per Quote** by ad asset type
- **Cost Per Result** by campaign objective
- **Quote Rates**, **Lead Rates**, **CTR (%)**, **Instagram Visit Rates**
- Automatically calculates time-based performance summaries.

All outputs are stored in `reports/` with timestamped filenames.

---

## ğŸ”® Future Additions

- âœ¨ Power BI / QuickSight Dashboards
- ğŸ“ˆ Time series modeling for quote predictions
- ğŸ§  Embedding performance metrics for LLM-powered campaign summaries
- â³ Append vs. Table-per-batch strategies for advanced archiving

---

## ğŸ§  Who Is This For?

This platform is ideal for:
- âœ¨ **Marketing teams** running frequent META ad campaigns
- ğŸ“Š **Data scientists** looking to automate social ad analytics
- ğŸ§ª **Startups** building pipelines around paid acquisition
- ğŸ•µï¸ **Analysts** tired of manually wrangling ad CSVs every Monday

---

## ğŸ¤ Contributing

Pull requests are welcome! If youâ€™ve got new metrics, visualization templates, or data integrations, send â€˜em in. Letâ€™s build the **Ad Oracle** we deserve.

---

## ğŸ§™â€â™‚ï¸ Maintainer

Crafted by [Keith Arogo Owino](https://keitharogo.github.io/)
